<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/zhongsheng.jpg"/><link rel="stylesheet" href="/_next/static/css/51bc899147ec0689.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-af78f5c2c11af041.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-bb0149f8815d6476.js" async=""></script><script src="/_next/static/chunks/408-464634266c54e89f.js" async=""></script><script src="/_next/static/chunks/874-7c5eb7db8d3a51f7.js" async=""></script><script src="/_next/static/chunks/55-328b1d72b5e3e498.js" async=""></script><script src="/_next/static/chunks/app/layout-fbe39f4a18972378.js" async=""></script><script src="/_next/static/chunks/63-0d5ebfce210dba9a.js" async=""></script><script src="/_next/static/chunks/486-d2ce1eaba6fd711e.js" async=""></script><script src="/_next/static/chunks/147-9a4bc6658f595514.js" async=""></script><script src="/_next/static/chunks/748-bce208fe0b98aa28.js" async=""></script><script src="/_next/static/chunks/app/page-51aa2e0f9e0de35e.js" async=""></script><link rel="icon" href="/fav.png" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Zhongsheng Wang ç‹é’Ÿå£°</title><meta name="description" content="Second-year PhD student at University of Auckland."/><meta name="author" content="Zhongsheng Wang"/><meta name="keywords" content="Zhongsheng Wang,PhD,Research,University of Auckland"/><meta name="creator" content="Zhongsheng Wang"/><meta name="publisher" content="Zhongsheng Wang"/><meta property="og:title" content="Zhongsheng Wang ç‹é’Ÿå£°"/><meta property="og:description" content="Second-year PhD student at University of Auckland."/><meta property="og:site_name" content="Zhongsheng Wang&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Zhongsheng Wang ç‹é’Ÿå£°"/><meta name="twitter:description" content="Second-year PhD student at University of Auckland."/><link rel="icon" href="/fav.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Zhongsheng Wang ç‹é’Ÿå£°</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/teaching/"><span class="relative z-10">Teaching</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a><div class="relative"><button type="button" class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm inline-flex items-center gap-1 text-neutral-600 hover:text-primary" aria-expanded="false" aria-label="Open post menu"><span class="relative z-10">Post</span><div class="relative z-10"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="m19.5 8.25-7.5 7.5-7.5-7.5"></path></svg></div></button></div></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Zhongsheng Wang" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/zhongsheng.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Zhongsheng Wang</h1><p class="text-lg text-accent font-medium mb-1">PhD Student</p><p class="text-neutral-600 mb-2">University of Auckland</p></div><div class="flex justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?user=JfrMQhsAAAAJ&amp;hl=zh-CN&amp;authuser=1" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://orcid.org/0009-0003-4235-7710" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="ORCID"><svg viewBox="0 0 24 24" fill="currentColor" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path d="M12 0C5.372 0 0 5.372 0 12s5.372 12 12 12 12-5.372 12-12S18.628 0 12 0zM7.369 4.378c.525 0 .947.431.947.947s-.422.947-.947.947a.95.95 0 0 1-.947-.947c0-.525.422-.947.947-.947zm-.722 3.038h1.444v10.041H6.647V7.416zm3.562 0h3.9c3.712 0 5.344 2.653 5.344 5.025 0 2.578-2.016 5.025-5.325 5.025h-3.919V7.416zm1.444 1.303v7.444h2.297c3.272 0 4.022-2.484 4.022-3.722 0-2.016-1.284-3.722-4.097-3.722h-2.222z"></path></svg></a><a href="https://github.com/Wzs010429" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://www.linkedin.com/in/zhongsheng-wang-095804278/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-5 w-5" aria-hidden="true"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a><a href="https://space.bilibili.com/22035617?spm_id_from=333.1007.0.0" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Bilibili"><svg viewBox="0 0 1024 1024" aria-hidden="true" class="h-5 w-5" xmlns="http://www.w3.org/2000/svg"><path fill="currentColor" d="M306.005333 117.632L444.330667 256h135.296l138.368-138.325333a42.666667 42.666667 0 1 1 60.373333 60.373333l-78.037333 77.952L789.333333 256A149.333333 149.333333 0 0 1 938.666667 405.333333v341.333334a149.333333 149.333333 0 0 1-149.333334 149.333333h-554.666666A149.333333 149.333333 0 0 1 85.333333 746.666667v-341.333334A149.333333 149.333333 0 0 1 234.666667 256h88.96L245.632 177.962667a42.666667 42.666667 0 0 1 60.373333-60.373334zM789.333333 341.333333h-554.666666a64 64 0 0 0-63.701334 57.856L170.666667 405.333333v341.333334a64 64 0 0 0 57.856 63.701333L234.666667 810.666667h554.666666a64 64 0 0 0 63.701334-57.813334L853.333333 746.666667v-341.333334A64 64 0 0 0 789.333333 341.333333zM341.333333 469.333333a42.666667 42.666667 0 0 1 42.666667 42.666667v85.333333a42.666667 42.666667 0 1 1-85.333333 0v-85.333333a42.666667 42.666667 0 0 1 42.666666-42.666667z m341.333334 0a42.666667 42.666667 0 0 1 42.666666 42.666667v85.333333a42.666667 42.666667 0 1 1-85.333333 0v-85.333333a42.666667 42.666667 0 0 1 42.666667-42.666667z"></path></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>MLLM</div><div>AIGC</div><div>Self-Evolving AI Agent</div><div>Multi-agent System</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><div class="mb-4 flex items-center gap-2"><h2 class="text-2xl font-serif font-bold text-primary">About</h2><button type="button" class="inline-flex h-8 w-8 items-center justify-center rounded-md transition-opacity hover:opacity-80" aria-label="åˆ‡æ¢åˆ°ä¸­æ–‡" title="åˆ‡æ¢åˆ°ä¸­æ–‡"><img alt="Translate" loading="lazy" width="18" height="18" decoding="async" data-nimg="1" class="h-[18px] w-[18px]" style="color:transparent" src="/translate_icon.png"/></button></div><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><div style="opacity:1;transform:none"><p class="mb-4 last:mb-0">I am a PhD Candidate in CS in the School of Computer Science, University of Auckland, supervised by <a href="https://profiles.auckland.ac.nz/jiamou-liu" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Jiamou Liu (åˆ˜ä½³è°‹)</a> and <a href="https://profiles.auckland.ac.nz/liu-qian" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Dr. Qian Liu (åˆ˜èŒœ)</a>.</p>
<p class="mb-4 last:mb-0">Before that, I received my undergraduate degree in Computer Science and Technology at <a href="https://cis.swu.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">the School of Computer Science and Technology, School of Software, Southwest University</a> in 2023 and master degree in Data Science at <a href="https://www.auckland.ac.nz/en/science/about-the-faculty/school-of-computer-science.html" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">the School of Computer Science, University of Auckland</a> in 2024.</p>
<p class="mb-4 last:mb-0">My current research interests lie in AIGC (especially the consistency and continuity of Text-to-Image generation), and the Self-Evolution of AI Agents.</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All â†’</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Narratology meets text-to-image: a survey of consistency in AI generated storybook illustrations</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class=" ">Zhedong Lin</span>, </span><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Qian Liu</span>, </span><span><span class=" ">Xinyu Zhang</span>, </span><span><span class=" ">Jiamou Liu</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Artificial Intelligence Review</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">Provides a comprehensive survey that grounds AIâ€‘generated storybook illustration in narratology, defining six dimensions of consistency and reviewing methods, datasets, metrics and challenges across time, space, character, plot, style and theme.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">CharCom: Composable Identity Control for Multi-Character Story Illustration</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Ming Lin</span>, </span><span><span class=" ">Zhedong Lin</span>, </span><span><span class=" ">Yaser Shakib</span>, </span><span><span class=" ">Qian Liu</span>, </span><span><span class=" ">Jiamou Liu</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Proceedings of the 7th ACM International Conference on Multimedia in Asia</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Chatlogic: Integrating logic programming with large language models for multi-step reasoning</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Jiamou Liu</span>, </span><span><span class=" ">Qiming Bao</span>, </span><span><span class=" ">Hongfei Rong</span>, </span><span><span class=" ">Jingfeng Zhang</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">2024 International Joint Conference on Neural Networks (IJCNN)</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">This paper proposes a bidirectional transformation method that converts natural language inference into logical code to enhance the multi-step reasoning ability of LLM.</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">News</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/news/">View All â†’</a></div><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-12</span><p class="text-sm text-neutral-700">ğŸ‰ One paper was accepted by AAMAS 2026 (co-1st author), thanks to all contributors!</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-10</span><p class="text-sm text-neutral-700">ğŸ‰ One paper was accepted by IJCNLP-AACL 2025, thanks to all contributors!</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-10</span><p class="text-sm text-neutral-700">ğŸ‰ One paper was accepted by ACM Multimedia Asia 2025 (1st author), see you in Malaysia!</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-03</span><p class="text-sm text-neutral-700">ğŸ‰ I just accepted my PhD offer and plan to start my PhD studies in April.</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-12</span><p class="text-sm text-neutral-700">ğŸ“ Visit UESTC at the invitation of Prof. Toru Takisaka, and had a small talk about LLM reasoning &amp; model checking.</p></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-11</span><p class="text-sm text-neutral-700">ğŸ“ Second trip to Japan! Attending PAKW 2024 (Kyoto Conference), Osaka, Mt. Fuji, Tokyoâ€¦</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="relative flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->February 15, 2026</p><p class="text-xs text-neutral-500 sm:absolute sm:left-1/2 sm:-translate-x-1/2">æå®¢æ°¸ä¸è®¤è¾“ Â· Still compiling... ğŸ”¥</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ğŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-af78f5c2c11af041.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"55\",\"static/chunks/55-328b1d72b5e3e498.js\",\"177\",\"static/chunks/app/layout-fbe39f4a18972378.js\"],\"ThemeProvider\"]\n3:I[768,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"55\",\"static/chunks/55-328b1d72b5e3e498.js\",\"177\",\"static/chunks/app/layout-fbe39f4a18972378.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"55\",\"static/chunks/55-328b1d72b5e3e498.js\",\"177\",\"static/chunks/app/layout-fbe39f4a18972378.js\"],\"default\"]\n7:I[7437,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"63\",\"static/chunks/63-0d5ebfce210dba9a.js\",\"486\",\"static/chunks/486-d2ce1eaba6fd711e.js\",\"147\",\"static/chunks/147-9a4bc6658f595514.js\",\"748\",\"static/chunks/748-bce208fe0b98aa28.js\",\"974\",\"static/chunks/app/page-51aa2e0f9e0de35e.js\"],\"default\"]\n8:I[9507,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"63\",\"static/chunks/63-0d5ebfce210dba9a.js\",\"486\",\"static/chunks/486-d2ce1eaba6fd711e.js\",\"147\",\"static/chunks/147-9a4bc6658f595514.js\",\"748\",\"static/chunks/748-bce208fe0b98aa28.js\",\"974\",\"static/chunks/app/page-51aa2e0f9e0de35e.js\"],\"default\"]\n9:I[5218,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"63\",\"static/chunks/63-0d5ebfce210dba9a.js\",\"486\",\"static/chunks/486-d2ce1eaba6fd711e.js\",\"147\",\"static/chunks/147-9a4bc6658f595514.js\",\"748\",\"static/chunks/748-bce208fe0b98aa28.js\",\"974\",\"static/chunks/app/page-51aa2e0f9e0de35e.js\"],\"default\"]\nf:I[1990,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"63\",\"static/chunks/63-0d5ebfce210dba9a.js\",\"486\",\"static/chunks/486-d2ce1eaba6fd711e.js\",\"147\",\"static/chunks/147-9a4bc6658f595514.js\",\"748\",\"static/chunks/748-bce208fe0b98aa28.js\",\"974\",\"static/chunks/app/page-51"])</script><script>self.__next_f.push([1,"aa2e0f9e0de35e.js\"],\"default\"]\n10:I[9665,[],\"MetadataBoundary\"]\n12:I[9665,[],\"OutletBoundary\"]\n15:I[4911,[],\"AsyncMetadataOutlet\"]\n17:I[9665,[],\"ViewportBoundary\"]\n19:I[6614,[],\"\"]\n:HL[\"/_next/static/css/51bc899147ec0689.css\",\"style\"]\na:T4f7,Textâ€‘toâ€‘image models are rapidly advancing into creative practice and increasingly support generating illustrated storybooks, i.e., sequential and imageâ€‘based narratives conditioned on written text. Previous surveys have examined challenges in video coherence or singleâ€‘image fidelity; however, no comprehensive review addresses the unique requirements of storybook illustration. This survey fills this gap by grounding AIâ€‘illustrated storybook generation in a narratology framework. Specifically, it introduces a sixâ€‘dimensional consistency model encompassing time, space, character, event and plot, style and theme. For each dimension the authors consolidate definitions, representative methods, datasets and evaluation metrics, mapping the current landscape of the field. Building on this analysis they further identify crossâ€‘dimensional failure modes and limitations of current approaches. Finally, the survey proposes potential future research directions, including the development of bookâ€‘scale integrated evaluation systems tailored for illustrated storybooks, more robust and controllable generation pipelines, enhanced multimodal semanticâ€“visual alignment mechanisms and the establishment of readerâ€‘oriented safety and educational guidelines.b:T6d2,@article{lin2026narratology,\n  title = {Narratology meets text-to-image: a survey of consistency in AI generated storybook illustrations},\n  author = {Lin, Zhedong and Wang, Zhongsheng and Liu, Qian and Zhang, Xinyu and Liu, Jiamou},\n  year = {2026},\n  journal = {Artificial Intelligence Review},\n  publisher = {Springer},\n  type = {journal},\n  status = {published},\n  researchArea = {machine-learning},\n  abstract = {Textâ€‘toâ€‘image models are rapidly advancing into creative practice and increasingly support generating illu"])</script><script>self.__next_f.push([1,"strated storybooks, i.e., sequential and imageâ€‘based narratives conditioned on written text. Previous surveys have examined challenges in video coherence or singleâ€‘image fidelity; however, no comprehensive review addresses the unique requirements of storybook illustration. This survey fills this gap by grounding AIâ€‘illustrated storybook generation in a narratology framework. Specifically, it introduces a sixâ€‘dimensional consistency model encompassing time, space, character, event and plot, style and theme. For each dimension the authors consolidate definitions, representative methods, datasets and evaluation metrics, mapping the current landscape of the field. Building on this analysis they further identify crossâ€‘dimensional failure modes and limitations of current approaches. Finally, the survey proposes potential future research directions, including the development of bookâ€‘scale integrated evaluation systems tailored for illustrated storybooks, more robust and controllable generation pipelines, enhanced multimodal semanticâ€“visual alignment mechanisms and the establishment of readerâ€‘oriented safety and educational guidelines.},\n  doi = {https://doi.org/10.1007/s10462-025-11482-6}\n}c:T6e7,@inproceedings{10.1145/3743093.3770985,\n  title = {CharCom: Composable Identity Control for Multi-Character Story Illustration},\n  author = {Wang, Zhongsheng and Lin, Ming and Lin, Zhedong and Shakib, Yaser and Liu, Qian and Liu, Jiamou},\n  year = {2025},\n  type = {conference},\n  status = {published},\n  tags = {Text-to-Image Generation, Diffusion Models, Low-Rank Adaptation, Adapter Composition, Identity Control},\n  researchArea = {machine-learning},\n  booktitle = {Proceedings of the 7th ACM International Conference on Multimedia in Asia},\n  doi = {10.1145/3743093.3770985},\n  url = {https://doi.org/10.1145/3743093.3770985},\n  abstract = {Ensuring character identity consistency across varying prompts remains a fundamental limitation in diffusion-based text-to-image generation. We propose CharCom, a modular and pa"])</script><script>self.__next_f.push([1,"rameter-efficient framework that achieves character-consistent story illustration through composable LoRA adapters, enabling efficient per-character customization without retraining the base model. Built on a frozen diffusion backbone, CharCom dynamically composes adapters at inference using prompt-aware control. Experiments on multi-scene narratives demonstrate that CharCom significantly enhances character fidelity, semantic alignment, and temporal coherence. It remains robust in crowded scenes and enables scalable multi-character generation with minimal overhead, making it well-suited for real-world applications such as story illustration and animation.},\n  video = {https://www.bilibili.com/video/BV1armaBhEHZ/?spm_id_from=333.1387.upload.video_card.click},\n  isbn = {9798400720055},\n  address = {New York, NY, USA},\n  publisher = {Association for Computing Machinery},\n  series = {MMAsia '25},\n  articleno = {41},\n  numpages = {7}\n}d:T479,Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated impressive capabilities in various generative tasks. However, their performance is often hampered by limitations in accessing and leveraging long-term memory, leading to specific vulnerabilities and biases, especially during long interactions. This paper introduces ChatLogic, an innovative framework specifically targeted at LLM reasoning tasks that can enhance the performance of LLMs in multi-step deductive reasoning tasks by integrating logic programming. In ChatLogic, the language model plays a central role, acting as a controller and participating in every system operation stage. We propose a novel method of converting logic problems into symbolic integration with an inference engine. This approach leverages large language models' situational understanding and imitation skills and uses symbolic memory to enhance multi-step deductive reasoning capabilities. Our results show that the ChatLogic framework significantly improves the multi-step reasoning capabilities of LLMs. The source code and data are availab"])</script><script>self.__next_f.push([1,"le at https://github.com/Strong-AI-Lab/ChatLogice:T70c,@inproceedings{wang2024chatlogic,\n  title = {Chatlogic: Integrating logic programming with large language models for multi-step reasoning},\n  author = {Wang, Zhongsheng and Liu, Jiamou and Bao, Qiming and Rong, Hongfei and Zhang, Jingfeng},\n  year = {2024},\n  type = {conference},\n  status = {published},\n  researchArea = {machine-learning},\n  booktitle = {2024 International Joint Conference on Neural Networks (IJCNN)},\n  pages = {1--8},\n  doi = {10.1109/IJCNN60899.2024.10650138},\n  url = {https://ieeexplore.ieee.org/document/10650138?denied=},\n  abstract = {Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated impressive capabilities in various generative tasks. However, their performance is often hampered by limitations in accessing and leveraging long-term memory, leading to specific vulnerabilities and biases, especially during long interactions. This paper introduces ChatLogic, an innovative framework specifically targeted at LLM reasoning tasks that can enhance the performance of LLMs in multi-step deductive reasoning tasks by integrating logic programming. In ChatLogic, the language model plays a central role, acting as a controller and participating in every system operation stage. We propose a novel method of converting logic problems into symbolic integration with an inference engine. This approach leverages large language models' situational understanding and imitation skills and uses symbolic memory to enhance multi-step deductive reasoning capabilities. Our results show that the ChatLogic framework significantly improves the multi-step reasoning capabilities of LLMs. The source code and data are available at https://github.com/Strong-AI-Lab/ChatLogic},\n  slides = {ChatLogicSlides.pdf},\n  poster = {ChatLogicPoster.pdf},\n  organization = {IEEE}\n}"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"I3sq1dxlKFClGm3POGcHm\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/51bc899147ec0689.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/fav.png\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Zhongsheng Wang ç‹é’Ÿå£°\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"February 15, 2026\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Zhongsheng Wang\",\"title\":\"PhD Student\",\"institution\":\"University of Auckland\",\"avatar\":\"/zhongsheng.jpg\"},\"social\":{\"email\":\"jkwzs0429@gmail.com\",\"location\":\"Auckland, New Zealand\",\"location_url\":\"https://maps.app.goo.gl/Zj8A8gkgmZ5Zrphr9\",\"location_details\":[\"Building 903\",\"314-390 Khyber Pass Road, Newmarket, Auckland 1023, New Zealand\"],\"google_scholar\":\"https://scholar.google.com/citations?user=JfrMQhsAAAAJ\u0026hl=zh-CN\u0026authuser=1\",\"orcid\":\"https://orcid.org/0009-0003-4235-7710\",\"github\":\"https://github.com/Wzs010429\",\"linkedin\":\"https://www.linkedin.com/in/zhongsheng-wang-095804278/\",\"bilibili\":\"https://space.bilibili.com/22035617?spm_id_from=333.1007.0.0\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"MLLM\",\"AIGC\",\"Self-Evolving AI Agent\",\"Multi-agent System\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"I am a PhD Candidate in CS in the School of Computer Science, University of Auckland, supervised by [Prof. Jiamou Liu (åˆ˜ä½³è°‹)](https://profiles.auckland.ac.nz/jiamou-liu) and [Dr. Qian Liu (åˆ˜èŒœ)](https://profiles.auckland.ac.nz/liu-qian).\\r\\n\\r\\nBefore that, I received my undergraduate degree in Computer Science and Technology at [the School of Computer Science and Technology, School of Software, Southwest University](https://cis.swu.edu.cn/) in 2023 and master degree in Data Science at [the School of Computer Science, University of Auckland](https://www.auckland.ac.nz/en/science/about-the-faculty/school-of-computer-science.html) in 2024.\\r\\n\\r\\nMy current research interests lie in AIGC (especially the consistency and continuity of Text-to-Image generation), and the Self-Evolution of AI Agents.\",\"contentZh\":\"æˆ‘ç›®å‰æ˜¯å¥¥å…‹å…°å¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢ï¼ˆSchool of Computer Scienceï¼‰çš„è®¡ç®—æœºç§‘å­¦äºŒå¹´çº§åšå£«ç ”ç©¶ç”Ÿï¼Œå¯¼å¸ˆä¸º [Prof. Jiamou Liuï¼ˆåˆ˜ä½³è°‹ï¼‰](https://profiles.auckland.ac.nz/jiamou-liu) ä¸ [Dr. Qian Liuï¼ˆåˆ˜èŒœï¼‰](https://profiles.auckland.ac.nz/liu-qian)ã€‚\\n\\nåœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘äº 2023 å¹´è·å¾—[è¥¿å—å¤§å­¦è®¡ç®—æœºä¸ä¿¡æ¯ç§‘å­¦å­¦é™¢ã€è½¯ä»¶å­¦é™¢](https://cis.swu.edu.cn/)è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ä¸“ä¸šå­¦å£«å­¦ä½ï¼Œå¹¶äº 2024 å¹´è·å¾—[å¥¥å…‹å…°å¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢](https://www.auckland.ac.nz/en/science/about-the-faculty/school-of-computer-science.html)æ•°æ®ç§‘å­¦ç¡•å£«å­¦ä½ã€‚\\n\\næˆ‘å½“å‰çš„ç ”ç©¶å…´è¶£ä¸»è¦åŒ…æ‹¬ AIGCï¼ˆå°¤å…¶æ˜¯æ–‡ç”Ÿå›¾ä¸­çš„ä¸€è‡´æ€§ä¸è¿ç»­æ€§é—®é¢˜ï¼‰ä»¥åŠ AI Agent çš„è‡ªè¿›åŒ–èƒ½åŠ›ã€‚\\n\",\"title\":\"About\"}],[\"$\",\"$L9\",\"featured_publications\",{\"publications\":[{\"id\":\"lin2026narratology\",\"title\":\"Narratology meets text-to-image: a survey of consistency in AI generated storybook illustrations\",\"authors\":[{\"name\":\"Zhedong Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qian Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xinyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Artificial Intelligence Review\",\"conference\":\"\",\"doi\":\"https://doi.org/10.1007/s10462-025-11482-6\",\"abstract\":\"$a\",\"description\":\"Provides a comprehensive survey that grounds AIâ€‘generated storybook illustration in narratology, defining six dimensions of consistency and reviewing methods, datasets, metrics and challenges across time, space, character, plot, style and theme.\",\"selected\":true,\"bibtex\":\"$b\"},{\"id\":\"10.1145/3743093.3770985\",\"title\":\"CharCom: Composable Identity Control for Multi-Character Story Illustration\",\"authors\":[{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ming Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhedong Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yaser Shakib\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qian Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 7th ACM International Conference on Multimedia in Asia\",\"doi\":\"10.1145/3743093.3770985\",\"url\":\"https://doi.org/10.1145/3743093.3770985\",\"abstract\":\"Ensuring character identity consistency across varying prompts remains a fundamental limitation in diffusion-based text-to-image generation. We propose CharCom, a modular and parameter-efficient framework that achieves character-consistent story illustration through composable LoRA adapters, enabling efficient per-character customization without retraining the base model. Built on a frozen diffusion backbone, CharCom dynamically composes adapters at inference using prompt-aware control. Experiments on multi-scene narratives demonstrate that CharCom significantly enhances character fidelity, semantic alignment, and temporal coherence. It remains robust in crowded scenes and enables scalable multi-character generation with minimal overhead, making it well-suited for real-world applications such as story illustration and animation.\",\"description\":\"\",\"selected\":true,\"preview\":\"ACMMMASIA2025.png\",\"bibtex\":\"$c\"},{\"id\":\"wang2024chatlogic\",\"title\":\"Chatlogic: Integrating logic programming with large language models for multi-step reasoning\",\"authors\":[{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qiming Bao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hongfei Rong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jingfeng Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"2024 International Joint Conference on Neural Networks (IJCNN)\",\"pages\":\"1--8\",\"doi\":\"10.1109/IJCNN60899.2024.10650138\",\"url\":\"https://ieeexplore.ieee.org/document/10650138?denied=\",\"code\":\"https://github.com/Strong-AI-Lab/ChatLogic\",\"abstract\":\"$d\",\"description\":\"This paper proposes a bidirectional transformation method that converts natural language inference into logical code to enhance the multi-step reasoning ability of LLM.\",\"selected\":true,\"preview\":\"chatlogic.png\",\"bibtex\":\"$e\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":false}],[\"$\",\"$Lf\",\"news\",{\"items\":[{\"date\":\"2025-12\",\"content\":\"ğŸ‰ One paper was accepted by AAMAS 2026 (co-1st author), thanks to all contributors!\"},{\"date\":\"2025-10\",\"content\":\"ğŸ‰ One paper was accepted by IJCNLP-AACL 2025, thanks to all contributors!\"},{\"date\":\"2025-10\",\"content\":\"ğŸ‰ One paper was accepted by ACM Multimedia Asia 2025 (1st author), see you in Malaysia!\"},{\"date\":\"2025-03\",\"content\":\"ğŸ‰ I just accepted my PhD offer and plan to start my PhD studies in April.\"},{\"date\":\"2024-12\",\"content\":\"ğŸ“ Visit UESTC at the invitation of Prof. Toru Takisaka, and had a small talk about LLM reasoning \u0026 model checking.\"},{\"date\":\"2024-11\",\"content\":\"ğŸ“ Second trip to Japan! Attending PAKW 2024 (Kyoto Conference), Osaka, Mt. Fuji, Tokyoâ€¦\"},{\"date\":\"2024-09\",\"content\":\"ğŸ¤— Obtain the Master's degree in Data Science from the School of Computer Science at the University of Auckland!\"},{\"date\":\"2024-08\",\"content\":\"ğŸ‰ One paper was accepted by ICONIP 2024, see u guys in Auckland this December!\"},{\"date\":\"2024-08\",\"content\":\"ğŸ“ PC Member for ICONIP 2024.\"},{\"date\":\"2024-07\",\"content\":\"ğŸ¤— Go to Yokohama, Japan to attend WCCI 2024 and present the paper on-site. Looking forward to meeting you guys!\"},{\"date\":\"2024-05\",\"content\":\"ğŸ“ As a local volunteer at AAMAS 2024 (New Zealand), looking forward to meeting more people and new inspirations.\"},{\"date\":\"2024-03\",\"content\":\"ğŸ‰ Two papers were accepted by WCCI 2024 (1st and 3rd author), the first time I published in the real sense.\"},{\"date\":\"2023-12\",\"content\":\"ğŸ‰ One paper was accepted by AAAI 2024 NucLeaR Workshop, which is the starting point of my scientific research career!\"}],\"title\":\"News\",\"limit\":6,\"showViewAll\":true,\"viewAllHref\":\"/news\"}]],false,false,false]}]]}]]}]}],[\"$\",\"$L10\",null,{\"children\":\"$L11\"}],null,[\"$\",\"$L12\",null,{\"children\":[\"$L13\",\"$L14\",[\"$\",\"$L15\",null,{\"promise\":\"$@16\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"4Friqjy4QyEM_Wj-CZARi\",{\"children\":[[\"$\",\"$L17\",null,{\"children\":\"$L18\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$19\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"1a:\"$Sreact.suspense\"\n1b:I[4911,[],\"AsyncMetadata\"]\n11:[\"$\",\"$1a\",null,{\"fallback\":null,\"children\":[\"$\",\"$L1b\",null,{\"promise\":\"$@1c\"}]}]\n"])</script><script>self.__next_f.push([1,"14:null\n"])</script><script>self.__next_f.push([1,"18:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n13:null\n"])</script><script>self.__next_f.push([1,"1c:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Zhongsheng Wang ç‹é’Ÿå£°\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Second-year PhD student at University of Auckland.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Zhongsheng Wang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Zhongsheng Wang,PhD,Research,University of Auckland\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Zhongsheng Wang\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Zhongsheng Wang\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Zhongsheng Wang ç‹é’Ÿå£°\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Second-year PhD student at University of Auckland.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Zhongsheng Wang's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Zhongsheng Wang ç‹é’Ÿå£°\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Second-year PhD student at University of Auckland.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/fav.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\n16:{\"metadata\":\"$1c:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>