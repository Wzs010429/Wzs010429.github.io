1:"$Sreact.fragment"
2:I[3719,["408","static/chunks/408-464634266c54e89f.js","874","static/chunks/874-7c5eb7db8d3a51f7.js","882","static/chunks/882-1a721ce788570045.js","177","static/chunks/app/layout-fe47f5010c9c1d0a.js"],"ThemeProvider"]
3:I[768,["408","static/chunks/408-464634266c54e89f.js","874","static/chunks/874-7c5eb7db8d3a51f7.js","882","static/chunks/882-1a721ce788570045.js","177","static/chunks/app/layout-fe47f5010c9c1d0a.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["408","static/chunks/408-464634266c54e89f.js","874","static/chunks/874-7c5eb7db8d3a51f7.js","882","static/chunks/882-1a721ce788570045.js","177","static/chunks/app/layout-fe47f5010c9c1d0a.js"],"default"]
7:I[7437,["408","static/chunks/408-464634266c54e89f.js","874","static/chunks/874-7c5eb7db8d3a51f7.js","63","static/chunks/63-0d5ebfce210dba9a.js","486","static/chunks/486-d2ce1eaba6fd711e.js","147","static/chunks/147-9a4bc6658f595514.js","748","static/chunks/748-bce208fe0b98aa28.js","974","static/chunks/app/page-51aa2e0f9e0de35e.js"],"default"]
8:I[9507,["408","static/chunks/408-464634266c54e89f.js","874","static/chunks/874-7c5eb7db8d3a51f7.js","63","static/chunks/63-0d5ebfce210dba9a.js","486","static/chunks/486-d2ce1eaba6fd711e.js","147","static/chunks/147-9a4bc6658f595514.js","748","static/chunks/748-bce208fe0b98aa28.js","974","static/chunks/app/page-51aa2e0f9e0de35e.js"],"default"]
9:I[5218,["408","static/chunks/408-464634266c54e89f.js","874","static/chunks/874-7c5eb7db8d3a51f7.js","63","static/chunks/63-0d5ebfce210dba9a.js","486","static/chunks/486-d2ce1eaba6fd711e.js","147","static/chunks/147-9a4bc6658f595514.js","748","static/chunks/748-bce208fe0b98aa28.js","974","static/chunks/app/page-51aa2e0f9e0de35e.js"],"default"]
f:I[1990,["408","static/chunks/408-464634266c54e89f.js","874","static/chunks/874-7c5eb7db8d3a51f7.js","63","static/chunks/63-0d5ebfce210dba9a.js","486","static/chunks/486-d2ce1eaba6fd711e.js","147","static/chunks/147-9a4bc6658f595514.js","748","static/chunks/748-bce208fe0b98aa28.js","974","static/chunks/app/page-51aa2e0f9e0de35e.js"],"default"]
10:I[9665,[],"MetadataBoundary"]
12:I[9665,[],"OutletBoundary"]
15:I[4911,[],"AsyncMetadataOutlet"]
17:I[9665,[],"ViewportBoundary"]
19:I[6614,[],""]
:HL["/_next/static/css/b288c494ad3c2a72.css","style"]
a:T4f7,Textâ€‘toâ€‘image models are rapidly advancing into creative practice and increasingly support generating illustrated storybooks, i.e., sequential and imageâ€‘based narratives conditioned on written text. Previous surveys have examined challenges in video coherence or singleâ€‘image fidelity; however, no comprehensive review addresses the unique requirements of storybook illustration. This survey fills this gap by grounding AIâ€‘illustrated storybook generation in a narratology framework. Specifically, it introduces a sixâ€‘dimensional consistency model encompassing time, space, character, event and plot, style and theme. For each dimension the authors consolidate definitions, representative methods, datasets and evaluation metrics, mapping the current landscape of the field. Building on this analysis they further identify crossâ€‘dimensional failure modes and limitations of current approaches. Finally, the survey proposes potential future research directions, including the development of bookâ€‘scale integrated evaluation systems tailored for illustrated storybooks, more robust and controllable generation pipelines, enhanced multimodal semanticâ€“visual alignment mechanisms and the establishment of readerâ€‘oriented safety and educational guidelines.b:T6d2,@article{lin2026narratology,
  title = {Narratology meets text-to-image: a survey of consistency in AI generated storybook illustrations},
  author = {Lin, Zhedong and Wang, Zhongsheng and Liu, Qian and Zhang, Xinyu and Liu, Jiamou},
  year = {2026},
  journal = {Artificial Intelligence Review},
  publisher = {Springer},
  type = {journal},
  status = {published},
  researchArea = {machine-learning},
  abstract = {Textâ€‘toâ€‘image models are rapidly advancing into creative practice and increasingly support generating illustrated storybooks, i.e., sequential and imageâ€‘based narratives conditioned on written text. Previous surveys have examined challenges in video coherence or singleâ€‘image fidelity; however, no comprehensive review addresses the unique requirements of storybook illustration. This survey fills this gap by grounding AIâ€‘illustrated storybook generation in a narratology framework. Specifically, it introduces a sixâ€‘dimensional consistency model encompassing time, space, character, event and plot, style and theme. For each dimension the authors consolidate definitions, representative methods, datasets and evaluation metrics, mapping the current landscape of the field. Building on this analysis they further identify crossâ€‘dimensional failure modes and limitations of current approaches. Finally, the survey proposes potential future research directions, including the development of bookâ€‘scale integrated evaluation systems tailored for illustrated storybooks, more robust and controllable generation pipelines, enhanced multimodal semanticâ€“visual alignment mechanisms and the establishment of readerâ€‘oriented safety and educational guidelines.},
  doi = {https://doi.org/10.1007/s10462-025-11482-6}
}c:T6e7,@inproceedings{10.1145/3743093.3770985,
  title = {CharCom: Composable Identity Control for Multi-Character Story Illustration},
  author = {Wang, Zhongsheng and Lin, Ming and Lin, Zhedong and Shakib, Yaser and Liu, Qian and Liu, Jiamou},
  year = {2025},
  type = {conference},
  status = {published},
  tags = {Text-to-Image Generation, Diffusion Models, Low-Rank Adaptation, Adapter Composition, Identity Control},
  researchArea = {machine-learning},
  booktitle = {Proceedings of the 7th ACM International Conference on Multimedia in Asia},
  doi = {10.1145/3743093.3770985},
  url = {https://doi.org/10.1145/3743093.3770985},
  abstract = {Ensuring character identity consistency across varying prompts remains a fundamental limitation in diffusion-based text-to-image generation. We propose CharCom, a modular and parameter-efficient framework that achieves character-consistent story illustration through composable LoRA adapters, enabling efficient per-character customization without retraining the base model. Built on a frozen diffusion backbone, CharCom dynamically composes adapters at inference using prompt-aware control. Experiments on multi-scene narratives demonstrate that CharCom significantly enhances character fidelity, semantic alignment, and temporal coherence. It remains robust in crowded scenes and enables scalable multi-character generation with minimal overhead, making it well-suited for real-world applications such as story illustration and animation.},
  video = {https://www.bilibili.com/video/BV1armaBhEHZ/?spm_id_from=333.1387.upload.video_card.click},
  isbn = {9798400720055},
  address = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  series = {MMAsia '25},
  articleno = {41},
  numpages = {7}
}d:T479,Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated impressive capabilities in various generative tasks. However, their performance is often hampered by limitations in accessing and leveraging long-term memory, leading to specific vulnerabilities and biases, especially during long interactions. This paper introduces ChatLogic, an innovative framework specifically targeted at LLM reasoning tasks that can enhance the performance of LLMs in multi-step deductive reasoning tasks by integrating logic programming. In ChatLogic, the language model plays a central role, acting as a controller and participating in every system operation stage. We propose a novel method of converting logic problems into symbolic integration with an inference engine. This approach leverages large language models' situational understanding and imitation skills and uses symbolic memory to enhance multi-step deductive reasoning capabilities. Our results show that the ChatLogic framework significantly improves the multi-step reasoning capabilities of LLMs. The source code and data are available at https://github.com/Strong-AI-Lab/ChatLogice:T70c,@inproceedings{wang2024chatlogic,
  title = {Chatlogic: Integrating logic programming with large language models for multi-step reasoning},
  author = {Wang, Zhongsheng and Liu, Jiamou and Bao, Qiming and Rong, Hongfei and Zhang, Jingfeng},
  year = {2024},
  type = {conference},
  status = {published},
  researchArea = {machine-learning},
  booktitle = {2024 International Joint Conference on Neural Networks (IJCNN)},
  pages = {1--8},
  doi = {10.1109/IJCNN60899.2024.10650138},
  url = {https://ieeexplore.ieee.org/document/10650138?denied=},
  abstract = {Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated impressive capabilities in various generative tasks. However, their performance is often hampered by limitations in accessing and leveraging long-term memory, leading to specific vulnerabilities and biases, especially during long interactions. This paper introduces ChatLogic, an innovative framework specifically targeted at LLM reasoning tasks that can enhance the performance of LLMs in multi-step deductive reasoning tasks by integrating logic programming. In ChatLogic, the language model plays a central role, acting as a controller and participating in every system operation stage. We propose a novel method of converting logic problems into symbolic integration with an inference engine. This approach leverages large language models' situational understanding and imitation skills and uses symbolic memory to enhance multi-step deductive reasoning capabilities. Our results show that the ChatLogic framework significantly improves the multi-step reasoning capabilities of LLMs. The source code and data are available at https://github.com/Strong-AI-Lab/ChatLogic},
  slides = {ChatLogicSlides.pdf},
  poster = {ChatLogicPoster.pdf},
  organization = {IEEE}
}0:{"P":null,"b":"7VDR9k-JpDIwbHyFvaHXU","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/b288c494ad3c2a72.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/fav.png","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"preload","as":"style","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","link",null,{"rel":"stylesheet","id":"gfonts-css","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap","media":"print"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              (function(){\n                var l = document.getElementById('gfonts-css');\n                if (!l) return;\n                if (l.media !== 'all') {\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\n                }\n              })();\n            "}}],["$","noscript",null,{"children":["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}]}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Teaching","type":"page","target":"teaching","href":"/teaching"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Zhongsheng Wang ç‹é’Ÿå£°","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"February 15, 2026"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Zhongsheng Wang","title":"PhD Student","institution":"University of Auckland","avatar":"/zhongsheng.jpg"},"social":{"email":"jkwzs0429@gmail.com","location":"Auckland, New Zealand","location_url":"https://maps.app.goo.gl/Zj8A8gkgmZ5Zrphr9","location_details":["Building 903","314-390 Khyber Pass Road, Newmarket, Auckland 1023, New Zealand"],"google_scholar":"https://scholar.google.com/citations?user=JfrMQhsAAAAJ&hl=zh-CN&authuser=1","orcid":"https://orcid.org/0009-0003-4235-7710","github":"https://github.com/Wzs010429","linkedin":"https://www.linkedin.com/in/zhongsheng-wang-095804278/","bilibili":"https://space.bilibili.com/22035617?spm_id_from=333.1007.0.0"},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["MLLM","AIGC","Self-Evolving AI Agent","Multi-agent System"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"I am a PhD Candidate in CS in the School of Computer Science, University of Auckland, supervised by [Prof. Jiamou Liu (åˆ˜ä½³è°‹)](https://profiles.auckland.ac.nz/jiamou-liu) and [Dr. Qian Liu (åˆ˜èŒœ)](https://profiles.auckland.ac.nz/liu-qian).\r\n\r\nBefore that, I received my undergraduate degree in Computer Science and Technology at [the School of Computer Science and Technology, School of Software, Southwest University](https://cis.swu.edu.cn/) in 2023 and master degree in Data Science at [the School of Computer Science, University of Auckland](https://www.auckland.ac.nz/en/science/about-the-faculty/school-of-computer-science.html) in 2024.\r\n\r\nMy current research interests lie in AIGC (especially the consistency and continuity of Text-to-Image generation), and the Self-Evolution of AI Agents.","contentZh":"æˆ‘ç›®å‰æ˜¯å¥¥å…‹å…°å¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢ï¼ˆSchool of Computer Scienceï¼‰çš„è®¡ç®—æœºç§‘å­¦äºŒå¹´çº§åšå£«ç ”ç©¶ç”Ÿï¼Œå¯¼å¸ˆä¸º [Prof. Jiamou Liuï¼ˆåˆ˜ä½³è°‹ï¼‰](https://profiles.auckland.ac.nz/jiamou-liu) ä¸ [Dr. Qian Liuï¼ˆåˆ˜èŒœï¼‰](https://profiles.auckland.ac.nz/liu-qian)ã€‚\n\nåœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘äº 2023 å¹´è·å¾—[è¥¿å—å¤§å­¦è®¡ç®—æœºä¸ä¿¡æ¯ç§‘å­¦å­¦é™¢ã€è½¯ä»¶å­¦é™¢](https://cis.swu.edu.cn/)è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ä¸“ä¸šå­¦å£«å­¦ä½ï¼Œå¹¶äº 2024 å¹´è·å¾—[å¥¥å…‹å…°å¤§å­¦è®¡ç®—æœºç§‘å­¦å­¦é™¢](https://www.auckland.ac.nz/en/science/about-the-faculty/school-of-computer-science.html)æ•°æ®ç§‘å­¦ç¡•å£«å­¦ä½ã€‚\n\næˆ‘å½“å‰çš„ç ”ç©¶å…´è¶£ä¸»è¦åŒ…æ‹¬ AIGCï¼ˆå°¤å…¶æ˜¯æ–‡ç”Ÿå›¾ä¸­çš„ä¸€è‡´æ€§ä¸è¿ç»­æ€§é—®é¢˜ï¼‰ä»¥åŠ AI Agent çš„è‡ªè¿›åŒ–èƒ½åŠ›ã€‚\n","title":"About"}],["$","$L9","featured_publications",{"publications":[{"id":"lin2026narratology","title":"Narratology meets text-to-image: a survey of consistency in AI generated storybook illustrations","authors":[{"name":"Zhedong Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhongsheng Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Qian Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Xinyu Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiamou Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2026,"type":"journal","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"Artificial Intelligence Review","conference":"","doi":"https://doi.org/10.1007/s10462-025-11482-6","abstract":"$a","description":"Provides a comprehensive survey that grounds AIâ€‘generated storybook illustration in narratology, defining six dimensions of consistency and reviewing methods, datasets, metrics and challenges across time, space, character, plot, style and theme.","selected":true,"bibtex":"$b"},{"id":"10.1145/3743093.3770985","title":"CharCom: Composable Identity Control for Multi-Character Story Illustration","authors":[{"name":"Zhongsheng Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Ming Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Zhedong Lin","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Yaser Shakib","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qian Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiamou Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2025,"type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"machine-learning","journal":"","conference":"Proceedings of the 7th ACM International Conference on Multimedia in Asia","doi":"10.1145/3743093.3770985","url":"https://doi.org/10.1145/3743093.3770985","abstract":"Ensuring character identity consistency across varying prompts remains a fundamental limitation in diffusion-based text-to-image generation. We propose CharCom, a modular and parameter-efficient framework that achieves character-consistent story illustration through composable LoRA adapters, enabling efficient per-character customization without retraining the base model. Built on a frozen diffusion backbone, CharCom dynamically composes adapters at inference using prompt-aware control. Experiments on multi-scene narratives demonstrate that CharCom significantly enhances character fidelity, semantic alignment, and temporal coherence. It remains robust in crowded scenes and enables scalable multi-character generation with minimal overhead, making it well-suited for real-world applications such as story illustration and animation.","description":"","selected":true,"preview":"ACMMMASIA2025.png","bibtex":"$c"},{"id":"wang2024chatlogic","title":"Chatlogic: Integrating logic programming with large language models for multi-step reasoning","authors":[{"name":"Zhongsheng Wang","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false},{"name":"Jiamou Liu","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Qiming Bao","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Hongfei Rong","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false},{"name":"Jingfeng Zhang","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false}],"year":2024,"type":"conference","status":"published","tags":[],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:2:tags","researchArea":"machine-learning","journal":"","conference":"2024 International Joint Conference on Neural Networks (IJCNN)","pages":"1--8","doi":"10.1109/IJCNN60899.2024.10650138","url":"https://ieeexplore.ieee.org/document/10650138?denied=","code":"https://github.com/Strong-AI-Lab/ChatLogic","abstract":"$d","description":"This paper proposes a bidirectional transformation method that converts natural language inference into logical code to enhance the multi-step reasoning ability of LLM.","selected":true,"preview":"chatlogic.png","bibtex":"$e"}],"title":"Selected Publications","enableOnePageMode":false}],["$","$Lf","news",{"items":[{"date":"2025-12","content":"ğŸ‰ One paper was accepted by AAMAS 2026 (co-1st author), thanks to all contributors!"},{"date":"2025-10","content":"ğŸ‰ One paper was accepted by IJCNLP-AACL 2025, thanks to all contributors!"},{"date":"2025-10","content":"ğŸ‰ One paper was accepted by ACM Multimedia Asia 2025 (1st author), see you in Malaysia!"},{"date":"2025-03","content":"ğŸ‰ I just accepted my PhD offer and plan to start my PhD studies in April."},{"date":"2024-12","content":"ğŸ“ Visit UESTC at the invitation of Prof. Toru Takisaka, and had a small talk about LLM reasoning & model checking."},{"date":"2024-11","content":"ğŸ“ Second trip to Japan! Attending PAKW 2024 (Kyoto Conference), Osaka, Mt. Fuji, Tokyoâ€¦"},{"date":"2024-09","content":"ğŸ¤— Obtain the Master's degree in Data Science from the School of Computer Science at the University of Auckland!"},{"date":"2024-08","content":"ğŸ‰ One paper was accepted by ICONIP 2024, see u guys in Auckland this December!"},{"date":"2024-08","content":"ğŸ“ PC Member for ICONIP 2024."},{"date":"2024-07","content":"ğŸ¤— Go to Yokohama, Japan to attend WCCI 2024 and present the paper on-site. Looking forward to meeting you guys!"},{"date":"2024-05","content":"ğŸ“ As a local volunteer at AAMAS 2024 (New Zealand), looking forward to meeting more people and new inspirations."},{"date":"2024-03","content":"ğŸ‰ Two papers were accepted by WCCI 2024 (1st and 3rd author), the first time I published in the real sense."},{"date":"2023-12","content":"ğŸ‰ One paper was accepted by AAAI 2024 NucLeaR Workshop, which is the starting point of my scientific research career!"}],"title":"News","limit":6,"showViewAll":true,"viewAllHref":"/news"}]],false,false,false]}]]}]]}]}],["$","$L10",null,{"children":"$L11"}],null,["$","$L12",null,{"children":["$L13","$L14",["$","$L15",null,{"promise":"$@16"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","oyl_0MIpmicMtrjaTNGhI",{"children":[["$","$L17",null,{"children":"$L18"}],null]}],null]}],false]],"m":"$undefined","G":["$19","$undefined"],"s":false,"S":true}
1a:"$Sreact.suspense"
1b:I[4911,[],"AsyncMetadata"]
11:["$","$1a",null,{"fallback":null,"children":["$","$L1b",null,{"promise":"$@1c"}]}]
14:null
18:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
13:null
1c:{"metadata":[["$","title","0",{"children":"Zhongsheng Wang ç‹é’Ÿå£°"}],["$","meta","1",{"name":"description","content":"Second-year PhD student at University of Auckland."}],["$","meta","2",{"name":"author","content":"Zhongsheng Wang"}],["$","meta","3",{"name":"keywords","content":"Zhongsheng Wang,PhD,Research,University of Auckland"}],["$","meta","4",{"name":"creator","content":"Zhongsheng Wang"}],["$","meta","5",{"name":"publisher","content":"Zhongsheng Wang"}],["$","meta","6",{"property":"og:title","content":"Zhongsheng Wang ç‹é’Ÿå£°"}],["$","meta","7",{"property":"og:description","content":"Second-year PhD student at University of Auckland."}],["$","meta","8",{"property":"og:site_name","content":"Zhongsheng Wang's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Zhongsheng Wang ç‹é’Ÿå£°"}],["$","meta","13",{"name":"twitter:description","content":"Second-year PhD student at University of Auckland."}],["$","link","14",{"rel":"icon","href":"/fav.png"}]],"error":null,"digest":"$undefined"}
16:{"metadata":"$1c:metadata","error":null,"digest":"$undefined"}
