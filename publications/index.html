<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/51bc899147ec0689.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-af78f5c2c11af041.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-bb0149f8815d6476.js" async=""></script><script src="/_next/static/chunks/408-464634266c54e89f.js" async=""></script><script src="/_next/static/chunks/874-7c5eb7db8d3a51f7.js" async=""></script><script src="/_next/static/chunks/55-328b1d72b5e3e498.js" async=""></script><script src="/_next/static/chunks/app/layout-eaee8c86285e04e2.js" async=""></script><script src="/_next/static/chunks/63-0d5ebfce210dba9a.js" async=""></script><script src="/_next/static/chunks/486-d2ce1eaba6fd711e.js" async=""></script><script src="/_next/static/chunks/147-9a4bc6658f595514.js" async=""></script><script src="/_next/static/chunks/748-bce208fe0b98aa28.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-40b7813e1f862592.js" async=""></script><link rel="icon" href="/fav.png" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Publications | Zhongsheng Wang ÁéãÈíüÂ£∞</title><meta name="description" content="A collection of my research work."/><meta name="author" content="Zhongsheng Wang"/><meta name="keywords" content="Zhongsheng Wang,PhD,Research,University of Auckland"/><meta name="creator" content="Zhongsheng Wang"/><meta name="publisher" content="Zhongsheng Wang"/><meta property="og:title" content="Zhongsheng Wang ÁéãÈíüÂ£∞"/><meta property="og:description" content="Second-year PhD student at University of Auckland."/><meta property="og:site_name" content="Zhongsheng Wang&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Zhongsheng Wang ÁéãÈíüÂ£∞"/><meta name="twitter:description" content="Second-year PhD student at University of Auckland."/><link rel="icon" href="/fav.png"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Zhongsheng Wang ÁéãÈíüÂ£∞</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/"><span class="relative z-10">About</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/publications/"><span class="relative z-10">Publications</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/teaching/"><span class="relative z-10">Teaching</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a><div class="relative"><button type="button" class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm inline-flex items-center gap-1 text-neutral-600 hover:text-primary" aria-expanded="false" aria-label="Open post menu"><span class="relative z-10">Post</span><div class="relative z-10"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="m19.5 8.25-7.5 7.5-7.5-7.5"></path></svg></div></button></div></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-¬´R5pdb¬ª" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12"><div style="opacity:0;transform:translateY(20px)"><div class="mb-8"><h1 class="text-4xl font-serif font-bold text-primary mb-4">Publications</h1><p class="text-lg text-neutral-600 dark:text-neutral-500 max-w-2xl">A collection of my research work.</p></div><div class="mb-8 space-y-4"><div class="flex flex-col sm:flex-row gap-4"><div class="relative flex-grow"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="absolute left-3 top-1/2 transform -translate-y-1/2 h-5 w-5 text-neutral-400"><path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path></svg><input type="text" placeholder="Search publications..." class="w-full pl-10 pr-4 py-2 rounded-lg border border-neutral-200 dark:border-neutral-800 bg-white dark:bg-neutral-900 focus:ring-2 focus:ring-accent focus:border-transparent transition-all duration-200" value=""/></div><button class="flex items-center justify-center px-4 py-2 rounded-lg border transition-all duration-200 bg-white dark:bg-neutral-900 border-neutral-200 dark:border-neutral-800 text-neutral-600 hover:border-accent hover:text-accent"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 mr-2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3c2.755 0 5.455.232 8.083.678.533.09.917.556.917 1.096v1.044a2.25 2.25 0 0 1-.659 1.591l-5.432 5.432a2.25 2.25 0 0 0-.659 1.591v2.927a2.25 2.25 0 0 1-1.244 2.013L9.75 21v-6.568a2.25 2.25 0 0 0-.659-1.591L3.659 7.409A2.25 2.25 0 0 1 3 5.818V4.774c0-.54.384-1.006.917-1.096A48.32 48.32 0 0 1 12 3Z"></path></svg>Filters</button></div></div><div class="space-y-6"><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Narratology meets text-to-image: a survey of consistency in AI generated storybook illustrations</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Zhedong Lin</span>, </span><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Qian Liu</span>, </span><span><span class=" ">Xinyu Zhang</span>, </span><span><span class=" ">Jiamou Liu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Artificial Intelligence Review<!-- --> <!-- -->2026</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">Provides a comprehensive survey that grounds AI‚Äëgenerated storybook illustration in narratology, defining six dimensions of consistency and reviewing methods, datasets, metrics and challenges across time, space, character, plot, style and theme.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/https://doi.org/10.1007/s10462-025-11482-6" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">LLM-based Business Process Models Generation from Textual Descriptions</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Xiaoxuan Li</span>, </span><span><span class=" ">Lin Ni</span>, </span><span><span class=" ">Xin Wang</span>, </span><span><span class=" ">Tang Yitong</span>, </span><span><span class=" ">Ruoxuan Li</span>, </span><span><span class=" ">Jiamou Liu</span>, </span><span><span class="font-semibold text-accent ">Zhongsheng Wang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics<!-- --> <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/R-debater.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Maoyuan Li</span>, </span><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Haoyuan Li</span>, </span><span><span class=" ">Jiamou Liu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">arXiv preprint arXiv:2512.24684<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">This article employs a multi-agent simulation approach, combined with relevant theoretical methods from debate, to implement an enhanced debate task, thereby improving the reliability of debate content.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="CharCom: Composable Identity Control for Multi-Character Story Illustration" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ACMMMASIA2025.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">CharCom: Composable Identity Control for Multi-Character Story Illustration</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Ming Lin</span>, </span><span><span class=" ">Zhedong Lin</span>, </span><span><span class=" ">Yaser Shakib</span>, </span><span><span class=" ">Qian Liu</span>, </span><span><span class=" ">Jiamou Liu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Proceedings of the 7th ACM International Conference on Multimedia in Asia<!-- --> <!-- -->2025</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1145/3743093.3770985" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Chatlogic: Integrating logic programming with large language models for multi-step reasoning" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/chatlogic.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Chatlogic: Integrating logic programming with large language models for multi-step reasoning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Jiamou Liu</span>, </span><span><span class=" ">Qiming Bao</span>, </span><span><span class=" ">Hongfei Rong</span>, </span><span><span class=" ">Jingfeng Zhang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">2024 International Joint Conference on Neural Networks (IJCNN)<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">This paper proposes a bidirectional transformation method that converts natural language inference into logical code to enhance the multi-step reasoning ability of LLM.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/IJCNN60899.2024.10650138" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><a href="https://github.com/Strong-AI-Lab/ChatLogic" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Epic-Level Text Generation with LLM through Auto-prompted Reinforcement Learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Qianqian Qi</span>, </span><span><span class=" ">Lin Ni</span>, </span><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Libo Zhang</span>, </span><span><span class=" ">Jiamou Liu</span>, </span><span><span class=" ">Michael Witbrock</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">2024 International Joint Conference on Neural Networks (IJCNN)<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">This article implements a method for generating ultra-long novel texts, ensuring a rigorous structure and continuous content.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/IJCNN60899.2024.10650358" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Cora: Optimizing low-rank adaptation with common subspace of large language models</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class=" ">Xiaojun Xiao</span>, </span><span><span class=" ">Sen Shen</span>, </span><span><span class=" ">Qiming Bao</span>, </span><span><span class=" ">Hongfei Rong</span>, </span><span><span class=" ">Kairui Liu</span>, </span><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Jiamou Liu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">arXiv preprint arXiv:2409.02119<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">Introduces CoRA, a method for improving Low‚ÄëRank Adaptation (LoRA) by replacing one of its matrices with a shared subspace across tasks, reducing parameters while maintaining or improving fine‚Äëtuning performance.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent ">Zhongsheng Wang</span>, </span><span><span class=" ">Sijie Wang</span>, </span><span><span class=" ">Jia Wang</span>, </span><span><span class=" ">Yung-I Liang</span>, </span><span><span class=" ">Yuxi Zhang</span>, </span><span><span class=" ">Jiamou Liu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">International Conference on Neural Information Processing<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">Presents a weak‚Äësupervision strategy for fine‚Äëtuning automatic speech recognition models tailored to customer relationship management systems, leading to significant improvements in identifying customer voices in industry applications.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1007/978-981-96-7036-9_10" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Abstract</button><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="relative flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->February 15, 2026</p><p class="text-xs text-neutral-500 sm:absolute sm:left-1/2 sm:-translate-x-1/2">ÊûÅÂÆ¢Ê∞∏‰∏çËÆ§Ëæì ¬∑ Still compiling... üî•</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-af78f5c2c11af041.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"55\",\"static/chunks/55-328b1d72b5e3e498.js\",\"177\",\"static/chunks/app/layout-eaee8c86285e04e2.js\"],\"ThemeProvider\"]\n3:I[768,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"55\",\"static/chunks/55-328b1d72b5e3e498.js\",\"177\",\"static/chunks/app/layout-eaee8c86285e04e2.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"874\",\"static/chunks/874-7c5eb7db8d3a51f7.js\",\"55\",\"static/chunks/55-328b1d72b5e3e498.js\",\"177\",\"static/chunks/app/layout-eaee8c86285e04e2.js\"],\"default\"]\n8:I[9665,[],\"MetadataBoundary\"]\na:I[9665,[],\"OutletBoundary\"]\nd:I[4911,[],\"AsyncMetadataOutlet\"]\nf:I[9665,[],\"ViewportBoundary\"]\n11:I[6614,[],\"\"]\n:HL[\"/_next/static/css/51bc899147ec0689.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"biQj0mbBA_L7cP-6gjPZo\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"publications\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/51bc899147ec0689.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/fav.png\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Teaching\",\"type\":\"page\",\"target\":\"teaching\",\"href\":\"/teaching\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Zhongsheng Wang ÁéãÈíüÂ£∞\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"February 15, 2026\"}]]}]}]]}]]}],{\"children\":[[\"slug\",\"publications\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"Ysz2e59gd2INFYVWKYt25\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[4911,[],\"AsyncMetadata\"]\n15:I[6669,[\"408\",\"static/chunks/408-464634266c54e89f.js\",\"63\",\"static/chunks/63-0d5ebfce210dba9a.js\",\"486\",\"static/chunks/486-d2ce1eaba6fd711e.js\",\"147\",\"static/chunks/147-9a4bc6658f595514.js\",\"748\",\"static/chunks/748-bce208fe0b98aa28.js\",\"182\",\"static/chunks/app/%5Bslug%5D/page-40b7813e1f862592.js\"],\"default\"]\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n16:T4f7,Text‚Äëto‚Äëimage models are rapidly advancing into creative practice and increasingly support generating illustrated storybooks, i.e., sequential and image‚Äëbased narratives conditioned on written text. Previous surveys have examined challenges in video coherence or single‚Äëimage fidelity; however, no comprehensive review addresses the unique requirements of storybook illustration. This survey fills this gap by grounding AI‚Äëillustrated storybook generation in a narratology framework. Specifically, it introduces a six‚Äëdimensional consistency model encompassing time, space, character, event and plot, style and theme. For each dimension the authors consolidate definitions, representative methods, datasets and evaluation metrics, mapping the current landscape of the field. Building on this analysis they further identify cross‚Äëdimensional failure modes and limitations of current approaches. Finally, the survey proposes potential future research directions, including the development of book‚Äëscale integrated evaluation systems tailored for illustrated storybooks, more robust and controllable generation pipelines, enhanced multimodal semantic‚Äìvisual alignment mechanisms and the establishment of reader‚Äëoriented safety and educational guidelines.17:T6d2,@article{lin2026narratology,\n  title = {Narratology meets text-to-image: a survey of consistency in AI generated storybook illustrations},\n  author = {Lin, Zhedong and Wang, Zhongsheng and Liu, Qian and Zhang, Xinyu and Liu, Jiamou},\n  year = {2026},\n  journal = {Artificial Intelligence Review},\n  publisher = "])</script><script>self.__next_f.push([1,"{Springer},\n  type = {journal},\n  status = {published},\n  researchArea = {machine-learning},\n  abstract = {Text‚Äëto‚Äëimage models are rapidly advancing into creative practice and increasingly support generating illustrated storybooks, i.e., sequential and image‚Äëbased narratives conditioned on written text. Previous surveys have examined challenges in video coherence or single‚Äëimage fidelity; however, no comprehensive review addresses the unique requirements of storybook illustration. This survey fills this gap by grounding AI‚Äëillustrated storybook generation in a narratology framework. Specifically, it introduces a six‚Äëdimensional consistency model encompassing time, space, character, event and plot, style and theme. For each dimension the authors consolidate definitions, representative methods, datasets and evaluation metrics, mapping the current landscape of the field. Building on this analysis they further identify cross‚Äëdimensional failure modes and limitations of current approaches. Finally, the survey proposes potential future research directions, including the development of book‚Äëscale integrated evaluation systems tailored for illustrated storybooks, more robust and controllable generation pipelines, enhanced multimodal semantic‚Äìvisual alignment mechanisms and the establishment of reader‚Äëoriented safety and educational guidelines.},\n  doi = {https://doi.org/10.1007/s10462-025-11482-6}\n}18:T67f,@inproceedings{li-etal-2025-llm-based-business,\n  title = {LLM-based Business Process Models Generation from Textual Descriptions},\n  author = {Li, Xiaoxuan and Ni, Lin and Wang, Xin and Yitong, Tang and Li, Ruoxuan and Liu, Jiamou and Wang, Zhongsheng},\n  year = {2025},\n  month = {12},\n  type = {conference},\n  status = {published},\n  researchArea = {machine-learning},\n  booktitle = {Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics},\n  pages = {523--533},\n  url = "])</script><script>self.__next_f.push([1,"{https://aclanthology.org/2025.findings-ijcnlp.31/},\n  abstract = {Business process modeling has traditionally depended on manual efforts or rigid rule-based techniques, limiting scalability and flexibility. Recent progress in Large Language Models (LLMs) enables automatic generation of process models from text, yet a systematic evaluation remains lacking. This paper explores the ability of LLMs to produce structurally and semantically valid business process workflows using five approaches: zero-shot, zero-shot CoT, few-shot, few-shot CoT, and fine-tuning. We assess performance under increasing control-flow complexity (e.g., nested gateways, parallel branches) using the MaD dataset, and introduce a masked-input setting to test semantic robustness. Results show that while fine-tuning achieves the best accuracy, few-shot CoT excels in handling complex logic and incomplete inputs. These findings reveal the strengths and limits of LLMs in process modeling and offer practical guidance for enterprise Business Process Management (BPM) automation.}\n}19:T4ab,We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20"])</script><script>self.__next_f.push([1," experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.1a:T614,@inproceedings{li2025r,\n  title = {R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory},\n  author = {Li, Maoyuan and Wang, Zhongsheng and Li, Haoyuan and Liu, Jiamou},\n  year = {2025},\n  type = {conference},\n  status = {published},\n  researchArea = {machine-learning},\n  booktitle = {arXiv preprint arXiv:2512.24684},\n  abstract = {We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.}\n}1b:T6e7,@inproceedings{10.1145/3743093.3770985,\n  title = {CharCom: Composable Identity Control for Multi-Character Story Illustration},\n  author = {Wang, Zhongsheng and Lin, Ming and Lin, Zhedong and Shakib, Yaser and Liu, Qian and Liu, Jiamou},\n  year = {2025},\n  type "])</script><script>self.__next_f.push([1,"= {conference},\n  status = {published},\n  tags = {Text-to-Image Generation, Diffusion Models, Low-Rank Adaptation, Adapter Composition, Identity Control},\n  researchArea = {machine-learning},\n  booktitle = {Proceedings of the 7th ACM International Conference on Multimedia in Asia},\n  doi = {10.1145/3743093.3770985},\n  url = {https://doi.org/10.1145/3743093.3770985},\n  abstract = {Ensuring character identity consistency across varying prompts remains a fundamental limitation in diffusion-based text-to-image generation. We propose CharCom, a modular and parameter-efficient framework that achieves character-consistent story illustration through composable LoRA adapters, enabling efficient per-character customization without retraining the base model. Built on a frozen diffusion backbone, CharCom dynamically composes adapters at inference using prompt-aware control. Experiments on multi-scene narratives demonstrate that CharCom significantly enhances character fidelity, semantic alignment, and temporal coherence. It remains robust in crowded scenes and enables scalable multi-character generation with minimal overhead, making it well-suited for real-world applications such as story illustration and animation.},\n  video = {https://www.bilibili.com/video/BV1armaBhEHZ/?spm_id_from=333.1387.upload.video_card.click},\n  isbn = {9798400720055},\n  address = {New York, NY, USA},\n  publisher = {Association for Computing Machinery},\n  series = {MMAsia '25},\n  articleno = {41},\n  numpages = {7}\n}1c:T479,Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated impressive capabilities in various generative tasks. However, their performance is often hampered by limitations in accessing and leveraging long-term memory, leading to specific vulnerabilities and biases, especially during long interactions. This paper introduces ChatLogic, an innovative framework specifically targeted at LLM reasoning tasks that can enhance the performance of LLMs in multi-step deductive reasoning tasks by integrating logic programming. In"])</script><script>self.__next_f.push([1," ChatLogic, the language model plays a central role, acting as a controller and participating in every system operation stage. We propose a novel method of converting logic problems into symbolic integration with an inference engine. This approach leverages large language models' situational understanding and imitation skills and uses symbolic memory to enhance multi-step deductive reasoning capabilities. Our results show that the ChatLogic framework significantly improves the multi-step reasoning capabilities of LLMs. The source code and data are available at https://github.com/Strong-AI-Lab/ChatLogic1d:T70c,@inproceedings{wang2024chatlogic,\n  title = {Chatlogic: Integrating logic programming with large language models for multi-step reasoning},\n  author = {Wang, Zhongsheng and Liu, Jiamou and Bao, Qiming and Rong, Hongfei and Zhang, Jingfeng},\n  year = {2024},\n  type = {conference},\n  status = {published},\n  researchArea = {machine-learning},\n  booktitle = {2024 International Joint Conference on Neural Networks (IJCNN)},\n  pages = {1--8},\n  doi = {10.1109/IJCNN60899.2024.10650138},\n  url = {https://ieeexplore.ieee.org/document/10650138?denied=},\n  abstract = {Large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated impressive capabilities in various generative tasks. However, their performance is often hampered by limitations in accessing and leveraging long-term memory, leading to specific vulnerabilities and biases, especially during long interactions. This paper introduces ChatLogic, an innovative framework specifically targeted at LLM reasoning tasks that can enhance the performance of LLMs in multi-step deductive reasoning tasks by integrating logic programming. In ChatLogic, the language model plays a central role, acting as a controller and participating in every system operation stage. We propose a novel method of converting logic problems into symbolic integration with an inference engine. This approach leverages large language models' situational understanding and imitation skills and "])</script><script>self.__next_f.push([1,"uses symbolic memory to enhance multi-step deductive reasoning capabilities. Our results show that the ChatLogic framework significantly improves the multi-step reasoning capabilities of LLMs. The source code and data are available at https://github.com/Strong-AI-Lab/ChatLogic},\n  slides = {ChatLogicSlides.pdf},\n  poster = {ChatLogicPoster.pdf},\n  organization = {IEEE}\n}1e:T49d,In an era where the capabilities of large language models (LLM) like ChatGPT are transforming digital communication, the challenge of directing these tools to create extensive, coherent narratives on an epic-scale has emerged as a critical frontier. This study introduces a novel methodology that fuses the spontaneous story generation of LLMs with the precision of auto-prompted reinforcement learning for crafting epic-scale, coherent narratives. Our approach starts with generating a skeletal outline, followed by iterative expansion, and blending operations for maintaining structural coherence in long-form content. To train the reinforcement learning model efficiently, we introduce an environment simulator that leverages a database of historical LLM interactions, circumventing the limitations of direct LLM interactions. This method enhances the decision-making process of the RL agent, enabling more effective prompt selection and narrative flow in extended texts. We validate its effectiveness through experiments, demonstrating the model's ability to generate structured, narrative-driven text, thereby setting a new pathway towards AI-driven, large-scale storytelling.1f:T73a,@inproceedings{10650358,\n  title = {Epic-Level Text Generation with LLM through Auto-prompted Reinforcement Learning},\n  author = {Qi, Qianqian and Ni, Lin and Wang, Zhongsheng and Zhang, Libo and Liu, Jiamou and Witbrock, Michael},\n  year = {2024},\n  type = {conference},\n  status = {published},\n  tags = {Systematics, Fuses, Large language models, Refining, Neural networks, Reinforcement learning, Writing, GPT-3.5, story generation, reinforcement learning, auto-prompt},\n  r"])</script><script>self.__next_f.push([1,"esearchArea = {neural-networks},\n  booktitle = {2024 International Joint Conference on Neural Networks (IJCNN)},\n  pages = {1-8},\n  doi = {10.1109/IJCNN60899.2024.10650358},\n  abstract = {In an era where the capabilities of large language models (LLM) like ChatGPT are transforming digital communication, the challenge of directing these tools to create extensive, coherent narratives on an epic-scale has emerged as a critical frontier. This study introduces a novel methodology that fuses the spontaneous story generation of LLMs with the precision of auto-prompted reinforcement learning for crafting epic-scale, coherent narratives. Our approach starts with generating a skeletal outline, followed by iterative expansion, and blending operations for maintaining structural coherence in long-form content. To train the reinforcement learning model efficiently, we introduce an environment simulator that leverages a database of historical LLM interactions, circumventing the limitations of direct LLM interactions. This method enhances the decision-making process of the RL agent, enabling more effective prompt selection and narrative flow in extended texts. We validate its effectiveness through experiments, demonstrating the model's ability to generate structured, narrative-driven text, thereby setting a new pathway towards AI-driven, large-scale storytelling.}\n}20:T4c0,In fine‚Äëtuning large language models, conserving computational resources while maintaining effectiveness and improving outcomes within the same computational constraints is crucial. The Low‚ÄëRank Adaptation (LoRA) strategy balances efficiency and performance by reducing the number of trainable parameters and computational costs. However, most work on LoRA focuses on fine‚Äëtuning methodologies without exploring further compression. Since many of LoRA's parameters may still be superfluous, they can lead to unnecessary resource consumption. This paper proposes CoRA, which leverages shared knowledge to optimize LoRA training by substituting its matrix B with a"])</script><script>self.__next_f.push([1," common subspace from large models. Two approaches are explored: (1) freezing the substitute matrix to halve parameters while training the remaining matrix for specific tasks and (2) using the substitute matrix as an enhanced initial state for the original matrix, achieving improved results with the same number of parameters. Experiments show that the first approach matches the efficacy of original LoRA fine‚Äëtuning while being more efficient, and the second approach yields further improvements, demonstrating the effectiveness of the method.21:T665,@inproceedings{xiao2024cora,\n  title = {Cora: Optimizing low-rank adaptation with common subspace of large language models},\n  author = {Xiao, Xiaojun and Shen, Sen and Bao, Qiming and Rong, Hongfei and Liu, Kairui and Wang, Zhongsheng and Liu, Jiamou},\n  year = {2024},\n  type = {conference},\n  status = {published},\n  researchArea = {machine-learning},\n  booktitle = {arXiv preprint arXiv:2409.02119},\n  abstract = {In fine‚Äëtuning large language models, conserving computational resources while maintaining effectiveness and improving outcomes within the same computational constraints is crucial. The Low‚ÄëRank Adaptation (LoRA) strategy balances efficiency and performance by reducing the number of trainable parameters and computational costs. However, most work on LoRA focuses on fine‚Äëtuning methodologies without exploring further compression. Since many of LoRA's parameters may still be superfluous, they can lead to unnecessary resource consumption. This paper proposes CoRA, which leverages shared knowledge to optimize LoRA training by substituting its matrix B with a common subspace from large models. Two approaches are explored: (1) freezing the substitute matrix to halve parameters while training the remaining matrix for specific tasks and (2) using the substitute matrix as an enhanced initial state for the original matrix, achieving improved results with the same number of parameters. Experiments show that the first approach matches the efficacy of original LoRA"])</script><script>self.__next_f.push([1," fine‚Äëtuning while being more efficient, and the second approach yields further improvements, demonstrating the effectiveness of the method.}\n}22:T597,@inproceedings{wang2024weak,\n  title = {Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems},\n  author = {Wang, Zhongsheng and Wang, Sijie and Wang, Jia and Liang, Yung-I and Zhang, Yuxi and Liu, Jiamou},\n  year = {2024},\n  type = {conference},\n  status = {published},\n  researchArea = {machine-learning},\n  booktitle = {International Conference on Neural Information Processing},\n  pages = {138--152},\n  abstract = {In the design of customer relationship management (CRM) systems, accurately identifying customer types and offering personalized services are key to enhancing customer satisfaction and loyalty. However, this process faces the challenge of discerning customer voices and intentions, and general pre‚Äëtrained automatic speech recognition (ASR) models make it difficult to effectively address industry‚Äëspecific speech recognition tasks. To address this issue the authors propose a solution for fine‚Äëtuning industry‚Äëspecific ASR models using weak supervision, which significantly improves the performance of the fine‚Äëtuned ASR models in industry applications. Experimental results show that this method substantially improves the crucial auxiliary role of the ASR model in industry CRM systems, and the approach has also been adopted in actual industrial applications.},\n  slides = {ICONIP2024Slides.pdf},\n  doi = {10.1007/978-981-96-7036-9_10},\n  organization = {Springer}\n}"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12\",\"children\":[[\"$\",\"$L15\",null,{\"config\":{\"type\":\"publication\",\"title\":\"Publications\",\"description\":\"A collection of my research work.\",\"source\":\"publications.bib\"},\"publications\":[{\"id\":\"lin2026narratology\",\"title\":\"Narratology meets text-to-image: a survey of consistency in AI generated storybook illustrations\",\"authors\":[{\"name\":\"Zhedong Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qian Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xinyu Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Artificial Intelligence Review\",\"conference\":\"\",\"doi\":\"https://doi.org/10.1007/s10462-025-11482-6\",\"abstract\":\"$16\",\"description\":\"Provides a comprehensive survey that grounds AI‚Äëgenerated storybook illustration in narratology, defining six dimensions of consistency and reviewing methods, datasets, metrics and challenges across time, space, character, plot, style and theme.\",\"selected\":true,\"bibtex\":\"$17\"},{\"id\":\"li-etal-2025-llm-based-business\",\"title\":\"LLM-based Business Process Models Generation from Textual Descriptions\",\"authors\":[{\"name\":\"Xiaoxuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lin Ni\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xin Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Tang Yitong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruoxuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 14th International Joint Conference on Natural Language Processing and the 4th Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics\",\"pages\":\"523--533\",\"url\":\"https://aclanthology.org/2025.findings-ijcnlp.31/\",\"abstract\":\"Business process modeling has traditionally depended on manual efforts or rigid rule-based techniques, limiting scalability and flexibility. Recent progress in Large Language Models (LLMs) enables automatic generation of process models from text, yet a systematic evaluation remains lacking. This paper explores the ability of LLMs to produce structurally and semantically valid business process workflows using five approaches: zero-shot, zero-shot CoT, few-shot, few-shot CoT, and fine-tuning. We assess performance under increasing control-flow complexity (e.g., nested gateways, parallel branches) using the MaD dataset, and introduce a masked-input setting to test semantic robustness. Results show that while fine-tuning achieves the best accuracy, few-shot CoT excels in handling complex logic and incomplete inputs. These findings reveal the strengths and limits of LLMs in process modeling and offer practical guidance for enterprise Business Process Management (BPM) automation.\",\"description\":\"\",\"selected\":false,\"bibtex\":\"$18\"},{\"id\":\"li2025r\",\"title\":\"R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory\",\"authors\":[{\"name\":\"Maoyuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haoyuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"arXiv preprint arXiv:2512.24684\",\"abstract\":\"$19\",\"description\":\"This article employs a multi-agent simulation approach, combined with relevant theoretical methods from debate, to implement an enhanced debate task, thereby improving the reliability of debate content.\",\"selected\":false,\"preview\":\"R-debater.png\",\"bibtex\":\"$1a\"},{\"id\":\"10.1145/3743093.3770985\",\"title\":\"CharCom: Composable Identity Control for Multi-Character Story Illustration\",\"authors\":[{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ming Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhedong Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yaser Shakib\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qian Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Proceedings of the 7th ACM International Conference on Multimedia in Asia\",\"doi\":\"10.1145/3743093.3770985\",\"url\":\"https://doi.org/10.1145/3743093.3770985\",\"abstract\":\"Ensuring character identity consistency across varying prompts remains a fundamental limitation in diffusion-based text-to-image generation. We propose CharCom, a modular and parameter-efficient framework that achieves character-consistent story illustration through composable LoRA adapters, enabling efficient per-character customization without retraining the base model. Built on a frozen diffusion backbone, CharCom dynamically composes adapters at inference using prompt-aware control. Experiments on multi-scene narratives demonstrate that CharCom significantly enhances character fidelity, semantic alignment, and temporal coherence. It remains robust in crowded scenes and enables scalable multi-character generation with minimal overhead, making it well-suited for real-world applications such as story illustration and animation.\",\"description\":\"\",\"selected\":true,\"preview\":\"ACMMMASIA2025.png\",\"bibtex\":\"$1b\"},{\"id\":\"wang2024chatlogic\",\"title\":\"Chatlogic: Integrating logic programming with large language models for multi-step reasoning\",\"authors\":[{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qiming Bao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hongfei Rong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jingfeng Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"2024 International Joint Conference on Neural Networks (IJCNN)\",\"pages\":\"1--8\",\"doi\":\"10.1109/IJCNN60899.2024.10650138\",\"url\":\"https://ieeexplore.ieee.org/document/10650138?denied=\",\"code\":\"https://github.com/Strong-AI-Lab/ChatLogic\",\"abstract\":\"$1c\",\"description\":\"This paper proposes a bidirectional transformation method that converts natural language inference into logical code to enhance the multi-step reasoning ability of LLM.\",\"selected\":true,\"preview\":\"chatlogic.png\",\"bibtex\":\"$1d\"},{\"id\":\"10650358\",\"title\":\"Epic-Level Text Generation with LLM through Auto-prompted Reinforcement Learning\",\"authors\":[{\"name\":\"Qianqian Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lin Ni\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Libo Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Michael Witbrock\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:5:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"2024 International Joint Conference on Neural Networks (IJCNN)\",\"pages\":\"1-8\",\"doi\":\"10.1109/IJCNN60899.2024.10650358\",\"abstract\":\"$1e\",\"description\":\"This article implements a method for generating ultra-long novel texts, ensuring a rigorous structure and continuous content.\",\"selected\":false,\"bibtex\":\"$1f\"},{\"id\":\"xiao2024cora\",\"title\":\"Cora: Optimizing low-rank adaptation with common subspace of large language models\",\"authors\":[{\"name\":\"Xiaojun Xiao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sen Shen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qiming Bao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hongfei Rong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kairui Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:6:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"arXiv preprint arXiv:2409.02119\",\"abstract\":\"$20\",\"description\":\"Introduces CoRA, a method for improving Low‚ÄëRank Adaptation (LoRA) by replacing one of its matrices with a shared subspace across tasks, reducing parameters while maintaining or improving fine‚Äëtuning performance.\",\"selected\":false,\"bibtex\":\"$21\"},{\"id\":\"wang2024weak\",\"title\":\"Weak Supervision Techniques towards Enhanced ASR Models in Industry-level CRM Systems\",\"authors\":[{\"name\":\"Zhongsheng Wang\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sijie Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jia Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yung-I Liang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuxi Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiamou Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"conference\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$7:props:children:0:props:publications:7:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Neural Information Processing\",\"pages\":\"138--152\",\"doi\":\"10.1007/978-981-96-7036-9_10\",\"abstract\":\"In the design of customer relationship management (CRM) systems, accurately identifying customer types and offering personalized services are key to enhancing customer satisfaction and loyalty. However, this process faces the challenge of discerning customer voices and intentions, and general pre‚Äëtrained automatic speech recognition (ASR) models make it difficult to effectively address industry‚Äëspecific speech recognition tasks. To address this issue the authors propose a solution for fine‚Äëtuning industry‚Äëspecific ASR models using weak supervision, which significantly improves the performance of the fine‚Äëtuned ASR models in industry applications. Experimental results show that this method substantially improves the crucial auxiliary role of the ASR model in industry CRM systems, and the approach has also been adopted in actual industrial applications.\",\"description\":\"Presents a weak‚Äësupervision strategy for fine‚Äëtuning automatic speech recognition models tailored to customer relationship management systems, leading to significant improvements in identifying customer voices in industry applications.\",\"selected\":false,\"bibtex\":\"$22\"}]}],false,false]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Publications | Zhongsheng Wang ÁéãÈíüÂ£∞\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A collection of my research work.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Zhongsheng Wang\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Zhongsheng Wang,PhD,Research,University of Auckland\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Zhongsheng Wang\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Zhongsheng Wang\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Zhongsheng Wang ÁéãÈíüÂ£∞\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Second-year PhD student at University of Auckland.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Zhongsheng Wang's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Zhongsheng Wang ÁéãÈíüÂ£∞\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Second-year PhD student at University of Auckland.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/fav.png\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>